{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertConfig , BertModel, DistilBertModel, AutoModelForSequenceClassification, BertForNextSentencePrediction, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Set the device and load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact and Agrument Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part load dataset will be use to train and test BERT\n",
    "\n",
    "This dataset was created by our `llama_week_labeling.py` script using `unsloth/llama-3-8b-Instruct-bnb-4bit` model running locally\n",
    "\n",
    "This dataset need to be created using a week labeling, in this case using llama text generation model, since we can't find usable dataset. \n",
    "\n",
    "We use dataset from [`US Election 2020 - Presidential Debates`](https://www.kaggle.com/datasets/headsortails/us-election-2020-presidential-debates) collection and start to create the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>How you doing, man?</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Iâ€™m well.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Well, first of all, thank you for doing this a...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>The American people have a right to have a say...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Now, whatâ€™s at stake here is the Presidentâ€™s m...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>And that ended when we, in fact, passed the Af...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Heâ€™s elected to the next election.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Thatâ€™s simply not true.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Open discussion.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Number one, he knows what I proposed. What I p...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    speaker  \\\n",
       "0  Vice President Joe Biden   \n",
       "1  Vice President Joe Biden   \n",
       "2  Vice President Joe Biden   \n",
       "3  Vice President Joe Biden   \n",
       "4  Vice President Joe Biden   \n",
       "5  Vice President Joe Biden   \n",
       "6  Vice President Joe Biden   \n",
       "7  Vice President Joe Biden   \n",
       "8  Vice President Joe Biden   \n",
       "9  Vice President Joe Biden   \n",
       "\n",
       "                                           statement     label  \n",
       "0                                How you doing, man?  Argument  \n",
       "1                                          Iâ€™m well.  Argument  \n",
       "2  Well, first of all, thank you for doing this a...  Argument  \n",
       "3  The American people have a right to have a say...  Argument  \n",
       "4  Now, whatâ€™s at stake here is the Presidentâ€™s m...  Argument  \n",
       "5  And that ended when we, in fact, passed the Af...  Argument  \n",
       "6                 Heâ€™s elected to the next election.  Argument  \n",
       "7                            Thatâ€™s simply not true.  Argument  \n",
       "8                                   Open discussion.  Argument  \n",
       "9  Number one, he knows what I proposed. What I p...  Argument  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 563 entries, 0 to 562\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   speaker    563 non-null    object\n",
      " 1   statement  563 non-null    object\n",
      " 2   label      563 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 13.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Argument', 'Fact'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Thank you, Susan. Well, the American people ha...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Can you imagine if you knew on January 28th, a...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>â€¦ right to reelection based on this.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Absolutely. Whatever the vice president is cla...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>No. But Susan, this is important. And I want t...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Mr. Vice President, Iâ€™m speaking.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Iâ€™m speaking.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Thank you. So I want to ask the American peopl...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>â€¦ when your children-</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>â€¦ couldnâ€™t see your parents because you were a...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         speaker                                          statement     label\n",
       "0  Kamala Harris  Thank you, Susan. Well, the American people ha...  Argument\n",
       "1  Kamala Harris  Can you imagine if you knew on January 28th, a...  Argument\n",
       "2  Kamala Harris               â€¦ right to reelection based on this.  Argument\n",
       "3  Kamala Harris  Absolutely. Whatever the vice president is cla...  Argument\n",
       "4  Kamala Harris  No. But Susan, this is important. And I want t...  Argument\n",
       "5  Kamala Harris                  Mr. Vice President, Iâ€™m speaking.  Argument\n",
       "6  Kamala Harris                                      Iâ€™m speaking.  Argument\n",
       "7  Kamala Harris  Thank you. So I want to ask the American peopl...  Argument\n",
       "8  Kamala Harris                              â€¦ when your children-  Argument\n",
       "9  Kamala Harris  â€¦ couldnâ€™t see your parents because you were a...  Argument"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157 entries, 0 to 156\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   speaker    157 non-null    object\n",
      " 1   statement  157 non-null    object\n",
      " 2   label      157 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Argument', 'Fact'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_fact_agrument_1=pd.read_csv(os.path.join('../Datasets','us_debates','fact-agrument','1st_presidential_fact_agument.csv'))\n",
    "df_fact_agrument_2=pd.read_csv(os.path.join('../Datasets','us_debates','fact-agrument','vice_presidential_fact_agrument.csv'))\n",
    "\n",
    "# Some info about each dataset\n",
    "df_fact_agrument_1.head(10)\n",
    "df_fact_agrument_1.info()\n",
    "df_fact_agrument_1['label'].unique()\n",
    "\n",
    "df_fact_agrument_2.head(10)\n",
    "df_fact_agrument_2.info()\n",
    "df_fact_agrument_2['label'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also gonna include the `Politifact Fact Check` since the debate we get are more agruments that facts.\n",
    "\n",
    "We need to create a new dataset from this one with the same format as the one above, and concat the 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>label_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Pence</td>\n",
       "      <td>And Iâ€™m going to speak up on behalf of what th...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>\"Already we've identified $2 trillion in defic...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>â€¦ for the Recovery Act that brought America ba...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Because you in fact passed that, that was your...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Mr. Vice-</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>David Beckham</td>\n",
       "      <td>\"Swaziland has the highest rate of HIV infecti...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Quotes Mike Pence as saying that people with p...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cindy Oâ€™Laughlin</td>\n",
       "      <td>\"When comparing state by state, the data clear...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>\"We've recovered (from the recession) faster a...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>A lot of people, between drugs and alcohol and...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     speaker  \\\n",
       "0                 Mike Pence   \n",
       "1               Barack Obama   \n",
       "2              Kamala Harris   \n",
       "3   Vice President Joe Biden   \n",
       "4   Vice President Joe Biden   \n",
       "5              David Beckham   \n",
       "6             Facebook posts   \n",
       "7           Cindy Oâ€™Laughlin   \n",
       "8               Barack Obama   \n",
       "9  President Donald J. Trump   \n",
       "\n",
       "                                           statement     label  label_map  \n",
       "0  And Iâ€™m going to speak up on behalf of what th...  Argument          1  \n",
       "1  \"Already we've identified $2 trillion in defic...      Fact          0  \n",
       "2  â€¦ for the Recovery Act that brought America ba...  Argument          1  \n",
       "3  Because you in fact passed that, that was your...  Argument          1  \n",
       "4                                          Mr. Vice-      Fact          0  \n",
       "5  \"Swaziland has the highest rate of HIV infecti...      Fact          0  \n",
       "6  Quotes Mike Pence as saying that people with p...      Fact          0  \n",
       "7  \"When comparing state by state, the data clear...      Fact          0  \n",
       "8  \"We've recovered (from the recession) faster a...      Fact          0  \n",
       "9  A lot of people, between drugs and alcohol and...  Argument          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2020 entries, 0 to 2019\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   speaker    2020 non-null   object\n",
      " 1   statement  2020 non-null   object\n",
      " 2   label      2020 non-null   object\n",
      " 3   label_map  2020 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 63.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_polifact = pd.read_json(os.path.join('../Datasets','politifact_factcheck_data.json'), lines=True)\n",
    "# Cut dataset to randomly 1300 rows, to prevent to many facts\n",
    "df_polifact = df_polifact.sample(n=1300, random_state=42)\n",
    "\n",
    "# Dataset for training\n",
    "df_trainer_final=pd.DataFrame(columns=[\"speaker\", \"statement\", \"label\"])\n",
    "\n",
    "# We add the speaker but is not needed \n",
    "# (the speaker information was not use to prevent biases)\n",
    "df_trainer_final[\"speaker\"]=df_polifact[\"statement_originator\"]\n",
    "df_trainer_final[\"statement\"]=df_polifact[\"statement\"]\n",
    "df_trainer_final[\"label\"]=\"Fact\"\n",
    "\n",
    "# Concat with the 2 early datasets and shuffe dataset\n",
    "df_trainer_final = pd.concat([df_trainer_final, df_fact_agrument_1, df_fact_agrument_2])\n",
    "df_trainer_final = df_trainer_final.sample(frac=1, random_state=50).reset_index(drop=True)\n",
    "\n",
    "# At last we gonna map the lables\n",
    "label2id = {\"Fact\": 0, \"Argument\": 1}\n",
    "id2label = {v: k for k, v in label2id.items()}  # Reverse mapping\n",
    "df_trainer_final[\"label_map\"]=df_trainer_final[\"label\"].map(label2id)\n",
    "\n",
    "df_trainer_final.head(10)\n",
    "df_trainer_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "We decided to use `distilbert-base-uncased` model with the following train hyperparamenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our models hyperparameters\n",
    "bert_model_name = 'distilbert-base-uncased' # smaller bert model\n",
    "num_classes = 6\n",
    "max_length = 128\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "warmup_steps=500  # number of warmup steps for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    \n",
    "    # Make last report and last accuracy a global variable\n",
    "    logger.info(\"\\nðŸ“Š Classification Report:\\n%s\", classification_report(labels, predictions, digits=4))\n",
    "    logger.info(f\"Validation Accuracy: {report[\"accuracy\"]:.4f}\")\n",
    "\n",
    "    classification_report_metrics = {\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "    }\n",
    "    return classification_report_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fact_agrument_clasifier(model, statement_list: list, labels_list: list, tokenizer):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"statement\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Do a split with stratify to preserve class distribution\n",
    "    x_train, x_test, y_train, y_test = train_test_split(statement_list, labels_list, test_size=0.35, stratify=labels_list)\n",
    "\n",
    "    # Create new train and test dataset\n",
    "    train_data = Dataset.from_dict({\n",
    "        'statement': [t[0] for t in x_train],\n",
    "        'label': [int(label) for label in y_train]\n",
    "    })\n",
    "\n",
    "    test_data = Dataset.from_dict({\n",
    "        'statement': [t[0] for t in x_test],\n",
    "        'label': [int(label) for label in y_test]\n",
    "    })\n",
    "\n",
    "    # Map the train and text dataset with the tokeneizer\n",
    "    train_data = train_data.map(tokenize_function, batched=True)\n",
    "    test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Remove original text columns (keep only tokenized inputs)\n",
    "    train_data = train_data.remove_columns([\"statement\"])\n",
    "    test_data = test_data.remove_columns([\"statement\"])\n",
    "    \n",
    "    # Define training agruments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        save_total_limit=2,  # limit the total amount of checkpoints, delete the older checkpoints\n",
    "        eval_steps=100, # Perform evaluation every 100 steps\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        metric_for_best_model=\"precison\",  # Metric to use for selecting the best model\n",
    "        greater_is_better=True,  # Whether a higher value of the metric is better\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,  # training data\n",
    "        eval_dataset=test_data,  # evaluation data\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"../models/distilbert_fact_agrument_classifier\")\n",
    "    tokenizer.save_pretrained(\"../models/distilbert_fact_agrument_classifier\")\n",
    "\n",
    "    # Reload with new model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"../models/distilbert_fact_agrument_classifier\").to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"../models/distilbert_fact_agrument_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [00:00<00:00, 3548.54 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [00:00<00:00, 3425.26 examples/s]\n",
      " 17%|â–ˆâ–‹        | 275/1650 [03:45<18:49,  1.22it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 500/1650 [03:22<08:57,  2.14it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 500/1650 [03:22<08:57,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4786, 'grad_norm': 17.310762405395508, 'learning_rate': 2e-05, 'epoch': 3.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1000/1650 [06:50<04:38,  2.34it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1000/1650 [06:50<04:38,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3842, 'grad_norm': 2.7215335369110107, 'learning_rate': 1.1304347826086957e-05, 'epoch': 6.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1500/1650 [10:23<01:02,  2.41it/s]\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1500/1650 [10:23<01:02,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3543, 'grad_norm': 3.4246113300323486, 'learning_rate': 2.6086956521739132e-06, 'epoch': 9.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1649/1650 [11:24<00:00,  2.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1650/1650 [11:30<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 690.4151, 'train_samples_per_second': 19.018, 'train_steps_per_second': 2.39, 'train_loss': 0.400108115456321, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Load default model\n",
    "config = BertConfig.from_pretrained(bert_model_name, num_labels=len(label2id), label2id=label2id, id2label=id2label)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, config=config).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Training\n",
    "statement_train_list=df_trainer_final[\"statement\"].to_list()\n",
    "labels_list=df_trainer_final[\"label_map\"].to_list()\n",
    "\n",
    "train_fact_agrument_clasifier(model, statement_train_list, labels_list, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models perfomance\n",
    "To evaluate the model we gonna give some text samples from polifact website and some text from the debates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for text is Fact\n",
      "Sentiment for text is Argument\n",
      "Sentiment for text is Argument\n"
     ]
    }
   ],
   "source": [
    "# Reload new model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/distilbert_fact_agrument_classifier\").to(device)\n",
    "\n",
    "# Function to test the sentiment of a text\n",
    "def test_model(text, model, tokenizer):\n",
    "    labels = {0: \"Fact\", 1: \"Argument\"}\n",
    "    # Tokenize the text and add padding/truncation\n",
    "    encoding = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Make the model prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).item()  # Convert logits to predicted class index\n",
    "\n",
    "    # Output the prediction label based on the model's class index\n",
    "    predicted_label = labels[predictions]  # Map the predicted index to the label\n",
    "    print(f\"Sentiment for text is {predicted_label}\")\n",
    "\n",
    "\n",
    "# Test sentiment prediction\n",
    "test_model(\"The poverty rate decreased by 3% in the last two years\", model, tokenizer) # Fact\n",
    "test_model(\"FEMA sent $59M LAST WEEK to luxury hotels in New York City to house illegal migrantsâ€¦ That money is meant for American disaster relief.\", model, tokenizer) # Agrument\n",
    "test_model(\"But many people are catching it. Many people are getting this disease that was sent to us by China, and it shouldnâ€™t have been allowed to happen.\", model, tokenizer) # Agrument"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
